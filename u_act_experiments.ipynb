# u_act_digital_twin.py
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Any
import random
from collections import deque
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical
import gym
from gym import spaces
import time

class UACTEnvironment:
    """U-shaped Automated Container Terminal Environment"""
    
    def __init__(self, num_containers=40, num_agvs=5, num_ycs=2, num_qcs=2, terminal_width=100, terminal_length=300):
        self.num_containers = num_containers
        self.num_agvs = num_agvs
        self.num_ycs = num_ycs
        self.num_qcs = num_qcs
        self.terminal_width = terminal_width
        self.terminal_length = terminal_length
        
        # Equipment parameters
        self.agv_speed = 5.0  # m/s
        self.yc_speed = 5.0   # m/s
        self.qc_speed = 7.0   # m/s
        
        # Initialize equipment
        self.agvs = self._initialize_agvs()
        self.ycs = self._initialize_ycs()
        self.qcs = self._initialize_qcs()
        
        # Task queues
        self.import_tasks = []
        self.export_tasks = []
        self.completed_tasks = []
        
        # State variables
        self.current_time = 0
        self.makespan = 0
        self.total_waiting_time = 0
        self.terminal_congestion_index = 0
        
        # Generate container tasks
        self._generate_tasks()
        
        # State space dimensions (based on paper)
        self.state_dim = 20
        
    def _initialize_agvs(self):
        agvs = []
        for i in range(self.num_agvs):
            agv = {
                'id': i,
                'position': np.random.uniform(0, self.terminal_length, 2),
                'status': 'idle',  # idle, moving, loading, unloading
                'current_task': None,
                'queueing_time': 0,
                'completion_time': 0,
                'load_rate': 0.0
            }
            agvs.append(agv)
        return agvs
    
    def _initialize_ycs(self):
        ycs = []
        for i in range(self.num_ycs):
            yc = {
                'id': i,
                'position': np.array([25 + i*50, 50]),  # Fixed yard positions
                'status': 'idle',
                'current_task': None,
                'queue_length': 0,
                'completion_time': 0,
                'load_rate': 0.0
            }
            ycs.append(yc)
        return ycs
    
    def _initialize_qcs(self):
        qcs = []
        for i in range(self.num_qcs):
            qc = {
                'id': i,
                'position': np.array([10 + i*30, 10]),  # Quay side positions
                'status': 'idle',
                'current_task': None
            }
            qcs.append(qc)
        return qcs
    
    def _generate_tasks(self):
        """Generate import and export container tasks"""
        num_import = self.num_containers // 2
        num_export = self.num_containers - num_import
        
        # Import tasks (QC -> AGV -> YC -> ET)
        for i in range(num_import):
            task = {
                'id': f'import_{i}',
                'type': 'import',
                'status': 'pending',
                'current_stage': 'qc_waiting',
                'creation_time': self.current_time,
                'start_time': None,
                'completion_time': None,
                'assigned_agv': None,
                'assigned_yc': None,
                'remaining_time': np.random.uniform(100, 300),
                'location': np.random.uniform(0, self.terminal_length, 2)
            }
            self.import_tasks.append(task)
        
        # Export tasks (ET -> YC -> AGV -> QC)
        for i in range(num_export):
            task = {
                'id': f'export_{i}',
                'type': 'export',
                'status': 'pending',
                'current_stage': 'yc_waiting',
                'creation_time': self.current_time,
                'start_time': None,
                'completion_time': None,
                'assigned_agv': None,
                'assigned_yc': None,
                'remaining_time': np.random.uniform(100, 300),
                'location': np.random.uniform(0, self.terminal_length, 2)
            }
            self.export_tasks.append(task)
    
    def get_state(self):
        """Get current state representation (20 dimensions as in paper)"""
        state = np.zeros(self.state_dim)
        
        # 1-2: Number of import and export containers
        state[0] = len([t for t in self.import_tasks if t['status'] == 'pending'])
        state[1] = len([t for t in self.export_tasks if t['status'] == 'pending'])
        
        # 3-6: Completion rates for import and export tasks
        import_tasks = [t for t in self.import_tasks if t['start_time'] is not None]
        export_tasks = [t for t in self.export_tasks if t['start_time'] is not None]
        
        if import_tasks:
            completion_rates = [1 - (t['remaining_time'] / 300) for t in import_tasks]
            state[2] = np.mean(completion_rates)
            state[3] = np.std(completion_rates)
        
        if export_tasks:
            completion_rates = [1 - (t['remaining_time'] / 300) for t in export_tasks]
            state[4] = np.mean(completion_rates)
            state[5] = np.std(completion_rates)
        
        # 7-8: Remaining processing time
        all_tasks = import_tasks + export_tasks
        if all_tasks:
            remaining_times = [t['remaining_time'] for t in all_tasks]
            state[6] = np.mean(remaining_times)
            state[7] = np.std(remaining_times)
        
        # 9-12: Equipment loading rates
        agv_load_rates = [agv['load_rate'] for agv in self.agvs]
        yc_load_rates = [yc['load_rate'] for yc in self.ycs]
        
        if agv_load_rates:
            state[8] = np.mean(agv_load_rates)
            state[9] = np.std(agv_load_rates)
        
        if yc_load_rates:
            state[10] = np.mean(yc_load_rates)
            state[11] = np.std(yc_load_rates)
        
        # 13-16: AGV transfer point queues
        agv_queue_lengths = [yc['queue_length'] for yc in self.ycs]
        agv_waiting_times = [agv['queueing_time'] for agv in self.agvs]
        
        if agv_queue_lengths:
            state[12] = np.mean(agv_queue_lengths)
            state[13] = np.std(agv_queue_lengths)
        
        if agv_waiting_times:
            state[14] = np.mean(agv_waiting_times)
            state[15] = np.std(agv_waiting_times)
        
        # 17-20: ET transfer point queues (simplified)
        et_queue_lengths = [random.randint(0, 5) for _ in range(self.num_ycs)]
        et_waiting_times = [random.uniform(0, 50) for _ in range(self.num_ycs)]
        
        state[16] = np.mean(et_queue_lengths)
        state[17] = np.std(et_queue_lengths)
        state[18] = np.mean(et_waiting_times)
        state[19] = np.std(et_waiting_times)
        
        return state
    
    def step(self, action):
        """Execute one time step with given action"""
        old_state = self.get_state()
        
        # Apply scheduling rule based on action
        self._apply_scheduling_rule(action)
        
        # Update equipment and tasks
        self._update_equipment()
        self._update_tasks()
        
        # Calculate reward
        new_state = self.get_state()
        reward = self._calculate_reward(old_state, new_state)
        
        # Check if episode is done
        done = self._is_done()
        
        # Update metrics
        self.current_time += 1
        self._update_metrics()
        
        return new_state, reward, done, {}
    
    def _apply_scheduling_rule(self, action):
        """Apply composite scheduling rule based on action"""
        # Action corresponds to one of 6 composite rules from the paper
        rules = [
            ('LRT_C', 'EUT_A', 'FCFS_Y'),  # Rule 1
            ('SRT_C', 'EUT_A', 'FCFS_Y'),  # Rule 2
            ('LRT_C', 'SPT_A', 'FCFS_Y'),  # Rule 3
            ('SRT_C', 'SPT_A', 'FCFS_Y'),  # Rule 4
            ('LRT_C', 'NLR_A', 'FCFS_Y'),  # Rule 5
            ('SRT_C', 'NLR_A', 'FCFS_Y'),  # Rule 6
        ]
        
        task_rule, agv_rule, yc_rule = rules[action]
        
        # Apply task prioritization
        if task_rule == 'LRT_C':
            # Largest Remaining Time first
            tasks = sorted(self.import_tasks + self.export_tasks, 
                         key=lambda x: x['remaining_time'], reverse=True)
        else:  # SRT_C
            # Smallest Remaining Time first
            tasks = sorted(self.import_tasks + self.export_tasks, 
                         key=lambda x: x['remaining_time'])
        
        # Assign tasks to available AGVs and YCs
        for task in tasks:
            if task['status'] == 'pending':
                available_agvs = [agv for agv in self.agvs if agv['status'] == 'idle']
                available_ycs = [yc for yc in self.ycs if yc['status'] == 'idle']
                
                if available_agvs and available_ycs:
                    # Select AGV based on rule
                    if agv_rule == 'EUT_A':
                        agv = min(available_agvs, key=lambda x: x['completion_time'])
                    elif agv_rule == 'SPT_A':
                        agv = min(available_agvs, key=lambda x: 
                                np.linalg.norm(x['position'] - task['location']))
                    else:  # NLR_A
                        agv = min(available_agvs, key=lambda x: x['load_rate'])
                    
                    # Select YC based on rule (FCFS with ET priority)
                    yc = available_ycs[0]  # Simple FCFS
                    
                    # Assign task
                    task['assigned_agv'] = agv['id']
                    task['assigned_yc'] = yc['id']
                    task['status'] = 'assigned'
                    task['start_time'] = self.current_time
                    
                    agv['status'] = 'moving'
                    agv['current_task'] = task['id']
                    yc['status'] = 'busy'
                    yc['current_task'] = task['id']
    
    def _update_equipment(self):
        """Update equipment states"""
        for agv in self.agvs:
            if agv['status'] == 'moving' and agv['current_task']:
                # Simulate movement
                agv['queueing_time'] += 1
                agv['load_rate'] = min(1.0, agv['load_rate'] + 0.1)
            
            elif agv['status'] == 'idle':
                agv['load_rate'] = max(0.0, agv['load_rate'] - 0.05)
        
        for yc in self.ycs:
            if yc['status'] == 'busy':
                yc['queue_length'] = max(0, yc['queue_length'] - 1)
                yc['load_rate'] = min(1.0, yc['load_rate'] + 0.1)
            else:
                yc['load_rate'] = max(0.0, yc['load_rate'] - 0.05)
    
    def _update_tasks(self):
        """Update task progress"""
        for task in self.import_tasks + self.export_tasks:
            if task['status'] == 'assigned':
                task['remaining_time'] -= 1
                if task['remaining_time'] <= 0:
                    task['status'] = 'completed'
                    task['completion_time'] = self.current_time
                    self.completed_tasks.append(task)
                    
                    # Free up equipment
                    if task['assigned_agv'] is not None:
                        agv = self.agvs[task['assigned_agv']]
                        agv['status'] = 'idle'
                        agv['current_task'] = None
                        agv['completion_time'] = self.current_time
                    
                    if task['assigned_yc'] is not None:
                        yc = self.ycs[task['assigned_yc']]
                        yc['status'] = 'idle'
                        yc['current_task'] = None
                        yc['completion_time'] = self.current_time
    
    def _calculate_reward(self, old_state, new_state):
        """Calculate reward based on paper's equation (19)"""
        alpha = 0.5  # Weighting factor from paper
        
        # Change in completion rate
        delta_cr = (new_state[2] + new_state[4]) - (old_state[2] + old_state[4])
        
        # Change in AGV queueing time (penalty)
        delta_queue = (new_state[14] - old_state[14])
        
        # Normalize by number of containers and AGVs
        reward = alpha * (delta_cr / self.num_containers) - (1 - alpha) * (delta_queue / self.num_agvs)
        
        return reward
    
    def _is_done(self):
        """Check if all tasks are completed"""
        total_tasks = len(self.import_tasks) + len(self.export_tasks)
        completed = len(self.completed_tasks)
        return completed >= total_tasks * 0.95  # 95% completion
    
    def _update_metrics(self):
        """Update performance metrics"""
        if self.completed_tasks:
            completion_times = [t['completion_time'] for t in self.completed_tasks if t['completion_time']]
            if completion_times:
                self.makespan = max(completion_times)
        
        agv_queue_times = [agv['queueing_time'] for agv in self.agvs]
        self.total_waiting_time = sum(agv_queue_times)
        
        if self.makespan > 0:
            self.terminal_congestion_index = (self.total_waiting_time / 
                                            (self.num_agvs * self.makespan)) * 100
    
    def reset(self):
        """Reset environment to initial state"""
        self.__init__(self.num_containers, self.num_agvs, self.num_ycs, self.num_qcs,
                     self.terminal_width, self.terminal_length)
        return self.get_state()

class PPOAgent:
    """Proximal Policy Optimization Agent for U-ACT Scheduling"""
    
    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, epsilon=0.2):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon
        
        # Actor-Critic networks
        self.actor = ActorNetwork(state_dim, action_dim)
        self.critic = CriticNetwork(state_dim)
        
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)
        
        self.memory = []
        
    def get_action(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        probs = self.actor(state)
        dist = Categorical(probs)
        action = dist.sample()
        log_prob = dist.log_prob(action)
        
        return action.item(), log_prob
    
    def update(self, states, actions, log_probs, rewards, next_states, dones):
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        old_log_probs = torch.stack(log_probs)
        rewards = torch.FloatTensor(rewards)
        next_states = torch.FloatTensor(next_states)
        dones = torch.FloatTensor(dones)
        
        # Calculate advantages
        with torch.no_grad():
            values = self.critic(states)
            next_values = self.critic(next_states)
            advantages = rewards + self.gamma * next_values * (1 - dones) - values
        
        # Actor loss
        probs = self.actor(states)
        dist = Categorical(probs)
        new_log_probs = dist.log_prob(actions)
        
        ratio = (new_log_probs - old_log_probs).exp()
        surr1 = ratio * advantages
        surr2 = torch.clamp(ratio, 1 - self.epsilon, 1 + self.epsilon) * advantages
        
        actor_loss = -torch.min(surr1, surr2).mean()
        
        # Critic loss
        critic_loss = F.mse_loss(values, rewards + self.gamma * next_values * (1 - dones))
        
        # Update networks
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()

class ActorNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(ActorNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, x):
        return self.fc(x)

class CriticNetwork(nn.Module):
    def __init__(self, state_dim):
        super(CriticNetwork, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
    
    def forward(self, x):
        return self.fc(x)

class DigitalTwinVisualization:
    """Visualization tools for the Digital Twin environment"""
    
    def __init__(self, env):
        self.env = env
        self.fig, self.axes = plt.subplots(2, 2, figsize=(15, 10))
        plt.ion()
    
    def plot_terminal_layout(self):
        """Plot U-shaped terminal layout"""
        ax = self.axes[0, 0]
        ax.clear()
        
        # Draw U-shaped layout
        width = self.env.terminal_width
        length = self.env.terminal_length
        
        # Quay side
        ax.plot([0, length], [10, 10], 'b-', linewidth=3, label='Quay')
        
        # Yard blocks
        yard_width = 20
        for i in range(self.env.num_ycs):
            x = 25 + i * 50
            ax.add_patch(plt.Rectangle((x, 30), 40, yard_width, 
                                     facecolor='gray', alpha=0.5))
            ax.text(x + 20, 40, f'YC{i}', ha='center', va='center')
        
        # AGV paths
        ax.plot([0, length], [60, 60], 'r--', alpha=0.7, label='AGV Path')
        ax.plot([0, length], [70, 70], 'r--', alpha=0.7)
        
        # ET paths (U-shaped)
        ax.plot([0, length], [80, 80], 'g--', alpha=0.7, label='ET Path')
        ax.plot([length, length], [80, 90], 'g--', alpha=0.7)
        ax.plot([length, 0], [90, 90], 'g--', alpha=0.7)
        
        # Plot equipment positions
        for agv in self.env.agvs:
            ax.plot(agv['position'][0], agv['position'][1], 'ro', markersize=8, label='AGV')
        
        for yc in self.env.ycs:
            ax.plot(yc['position'][0], yc['position'][1], 'bs', markersize=10, label='YC')
        
        for qc in self.env.qcs:
            ax.plot(qc['position'][0], qc['position'][1], 'g^', markersize=10, label='QC')
        
        ax.set_xlim(0, length)
        ax.set_ylim(0, width)
        ax.set_title('U-Shaped Terminal Layout')
        ax.legend()
        ax.grid(True, alpha=0.3)
    
    def plot_performance_metrics(self, metrics_history):
        """Plot performance metrics over time"""
        ax = self.axes[0, 1]
        ax.clear()
        
        episodes = range(len(metrics_history))
        makespans = [m['makespan'] for m in metrics_history]
        tcis = [m['tci'] for m in metrics_history]
        rewards = [m['avg_reward'] for m in metrics_history]
        
        ax.plot(episodes, makespans, 'b-', label='Makespan')
        ax.set_ylabel('Makespan', color='b')
        ax.tick_params(axis='y', labelcolor='b')
        
        ax2 = ax.twinx()
        ax2.plot(episodes, tcis, 'r-', label='TCI')
        ax2.set_ylabel('Terminal Congestion Index (%)', color='r')
        ax2.tick_params(axis='y', labelcolor='r')
        
        ax.set_xlabel('Episode')
        ax.set_title('Performance Metrics')
        ax.grid(True, alpha=0.3)
        ax.legend(loc='upper left')
        ax2.legend(loc='upper right')
    
    def plot_equipment_utilization(self):
        """Plot equipment utilization rates"""
        ax = self.axes[1, 0]
        ax.clear()
        
        equipment = ['AGVs', 'YCs', 'QCs']
        utilization = [
            np.mean([agv['load_rate'] for agv in self.env.agvs]),
            np.mean([yc['load_rate'] for yc in self.env.ycs]),
            np.mean([qc.get('load_rate', 0.5) for qc in self.env.qcs])
        ]
        
        bars = ax.bar(equipment, utilization, color=['red', 'blue', 'green'], alpha=0.7)
        ax.set_ylabel('Utilization Rate')
        ax.set_title('Equipment Utilization')
        ax.set_ylim(0, 1)
        
        # Add value labels on bars
        for bar in bars:
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{height:.2f}', ha='center', va='bottom')
    
    def plot_task_progress(self):
        """Plot task completion progress"""
        ax = self.axes[1, 1]
        ax.clear()
        
        total_tasks = len(self.env.import_tasks) + len(self.env.export_tasks)
        completed = len(self.env.completed_tasks)
        pending = total_tasks - completed
        
        status = ['Completed', 'Pending']
        counts = [completed, pending]
        colors = ['green', 'orange']
        
        ax.pie(counts, labels=status, colors=colors, autopct='%1.1f%%', startangle=90)
        ax.set_title(f'Task Progress ({completed}/{total_tasks} completed)')
    
    def update_plots(self, metrics_history):
        """Update all plots"""
        self.plot_terminal_layout()
        self.plot_performance_metrics(metrics_history)
        self.plot_equipment_utilization()
        self.plot_task_progress()
        
        plt.tight_layout()
        plt.pause(0.01)

def train_ppo_agent():
    """Train PPO agent for U-ACT scheduling"""
    # Initialize environment and agent
    env = UACTEnvironment(num_containers=40, num_agvs=5)
    agent = PPOAgent(state_dim=env.state_dim, action_dim=6)
    
    # Training parameters
    num_episodes = 1000
    max_steps = 500
    update_interval = 50
    
    metrics_history = []
    
    print("Starting PPO training for U-ACT scheduling...")
    
    for episode in range(num_episodes):
        state = env.reset()
        episode_rewards = []
        states, actions, log_probs, rewards, next_states, dones = [], [], [], [], [], []
        
        for step in range(max_steps):
            # Get action from agent
            action, log_prob = agent.get_action(state)
            
            # Execute action
            next_state, reward, done, _ = env.step(action)
            
            # Store transition
            states.append(state)
            actions.append(action)
            log_probs.append(log_prob)
            rewards.append(reward)
            next_states.append(next_state)
            dones.append(done)
            
            state = next_state
            episode_rewards.append(reward)
            
            if done:
                break
        
        # Update agent
        if len(states) >= update_interval:
            agent.update(states, actions, log_probs, rewards, next_states, dones)
        
        # Calculate metrics
        avg_reward = np.mean(episode_rewards)
        metrics = {
            'episode': episode,
            'avg_reward': avg_reward,
            'makespan': env.makespan,
            'tci': env.terminal_congestion_index,
            'total_waiting': env.total_waiting_time
        }
        metrics_history.append(metrics)
        
        if episode % 100 == 0:
            print(f"Episode {episode}: Avg Reward = {avg_reward:.4f}, "
                  f"Makespan = {env.makespan}, TCI = {env.terminal_congestion_index:.2f}%")
    
    return env, agent, metrics_history

def compare_scheduling_rules():
    """Compare different scheduling rules"""
    rules = ['PPO', 'Rule1', 'Rule2', 'Rule3', 'Rule4', 'Rule5', 'Rule6']
    results = []
    
    for rule_name in rules:
        env = UACTEnvironment(num_containers=100, num_agvs=5)
        state = env.reset()
        
        if rule_name == 'PPO':
            # Use trained PPO agent
            agent = PPOAgent(state_dim=env.state_dim, action_dim=6)
            # In practice, you would load a trained model here
            actions = [random.randint(0, 5) for _ in range(200)]  # Placeholder
        else:
            # Use fixed rule
            rule_map = {'Rule1': 0, 'Rule2': 1, 'Rule3': 2, 'Rule4': 3, 'Rule5': 4, 'Rule6': 5}
            action = rule_map[rule_name]
            actions = [action] * 200
        
        for action in actions:
            state, reward, done, _ = env.step(action)
            if done:
                break
        
        results.append({
            'Rule': rule_name,
            'Makespan': env.makespan,
            'TCI': env.terminal_congestion_index,
            'Total Waiting': env.total_waiting_time
        })
    
    return pd.DataFrame(results)

if __name__ == "__main__":
    # Train the PPO agent
    env, agent, metrics_history = train_ppo_agent()
    
    # Compare scheduling rules
    comparison_df = compare_scheduling_rules()
    print("\nScheduling Rule Comparison:")
    print(comparison_df)
    
    # Visualize results
    viz = DigitalTwinVisualization(env)
    viz.update_plots(metrics_history)
    
    # Save visualization
    plt.savefig('u_act_scheduling_results.png', dpi=300, bbox_inches='tight')
    print("Results saved to 'u_act_scheduling_results.png'")